Metadata-Version: 2.1
Name: nlutools
Version: 1.19.7
Summary: nlu service tools
Home-page: https://www.ifchange.com
Author: ai3
Author-email: ai3@ifchange.com
License: Apache License 2.0
Description: # NLUTOOLS: NLU 工具包
        
        nlutools 是一系列模型与算法的nlu工具包，提供以下功能：
        
        1. [切词](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#1-%E5%88%87%E8%AF%8D)
        2. [切句](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#2-%E5%88%87%E5%8F%A5)
        3. [词向量](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#3-%E8%AF%8D%E5%90%91%E9%87%8F)
        4. [句向量](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#4-%E5%8F%A5%E5%90%91%E9%87%8F)
        5. [预训练模型](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#5-%E4%B8%8B%E8%BD%BD%E9%A2%84%E8%AE%AD%E7%BB%83%E4%B8%AD%E6%96%87%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B)
        6. [实体](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#6-%E5%AE%9E%E4%BD%93)
        7. [情感分析](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#7-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90)
        8. [关键字提取](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#8-%E5%85%B3%E9%94%AE%E5%AD%97%E6%8F%90%E5%8F%96)
        9. [句子相似性计算](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#9-%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97)
        10. [动宾提取](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#10-%E5%8A%A8%E5%AE%BE%E6%8F%90%E5%8F%96)
        11. [句子合理性判定](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#11-%E5%8F%A5%E5%AD%90%E5%90%88%E7%90%86%E6%80%A7%E5%88%A4%E5%88%AB)
        12. [姓名识别](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#12-%E5%A7%93%E5%90%8D%E8%AF%86%E5%88%AB%E6%9C%8D%E5%8A%A1)
        13. [小样本分类](https://gitlab.ifchange.com/nlu/nlutools/tree/dev/python#13-%E5%B0%8F%E6%A0%B7%E6%9C%AC%E5%88%86%E7%B1%BB)
        
        ## TODO:
        
        1. 切词增删自定义字典多进程和多实例没法在一次请求中完成配置
        
        ## 安装
        
        ```bash
        # 开发环境
        pip install -i http://211.148.28.23:59990/simple --trusted-host 211.148.28.23 nlutools
        # 线上/测试环境
        pip install -i http://pip.ifchange.com:59990/simple nlutools
        # pypi
        pip install -U nlutools
        # 源码安装
        git clone https://gitlab.ifchange.com/nlu/nlutools.git
        cd nlutools/python
        git checkout dev
        python setup.py develop
        ```
        
        ## 0 初始化
        
        ```python
        from nlutools import NLU
        # docker容器内必须手动指定运行环境，容器外可选
        # dev=开发，online_stable=线上服务，online_dev=线上调研，test=测试
        nlu = NLU(env="dev")
        # 若需要在测试环境测试模型相关服务的性能，请将env设置为online_dev
        # 线上服务建议切换为online_stable
        ```
        
        ## 1 切词
        
        切词工具接口函数：cut(text, mode, pos, cut_all)
        
        其中
        
        * text 为要切词的原始文本
        
        * mode 为分词模式，"fast"
        
        * pos为词性保留选项，True or False (默认开启)
        
        * cut_all为切词粒度控制 True or False (非百科名词短语支持，默认关闭)
        
        调用方式为：
        
        ```python
        nlu.cut('这是一个能够输出名词短语的分词器，欢迎试用！')
        ```
        
        返回结果：
        
        ```json
        {
            'np': ['分词器'],                   // 除去百度百科之外，其他的名词短语
            'entity' : ['名词短语'],            // 百度百科中会出现的词条
            'text': '这是一个能够输出名词短语的分词器，欢迎试用！',         // 原始文本
            'items' : ['这', '是', '一个', '能够', '输出', '名词短语', '的', '分词器', '，', '欢迎', '试用', '！'],  // 分词结果
            'pos': ['r', 'v', 'm', 'v', 'v', 'ne', 'uj', 'np', 'x', 'v', 'vn', 'x']     // 词性
        }
        ```
        
        ## 2 切句
        
        切句工具提供两种模式，接口函数：split(text, mode=0)
        
        其中
        
        * text 为需要进行切句的原始文本序列，格式为list
        
        * mode 为切句模式，默认值0(暂时只有这个默认值，忽略这个参数吧)
        
        调用方式：
        
        ```python
        nlu.split(['我喜欢在春天去观赏桃花。在夏天去欣赏荷花 在秋天去观赏红叶。但更喜欢在冬天去欣赏雪景。'])
        ```
        
        返回结果：
        
        ```json
        [['我喜欢在春天去观赏桃花', '在夏天去欣赏荷花 在秋天去观赏红叶', '但更喜欢在冬天去欣赏雪景']]
        ```
        
        ## 3 词向量
        
        3.1 离线词向量
        
           获得词向量文件，可以根据版本号获取，目前版本号包括：v1.0
        
           默认是下载最新版。获取到的文件夹下面包含两个文件，一个是词向量文件，一个是字向量文件。
        
        ```python
        nlu.getW2VFile('v1.0', '/local/path/')
        ```
        
        3.2 在线词向量
        
        支持两个来源的词向量，腾讯版(200维)和e成版(300维)，通过type参数控制('ifchange' or 'tencent', 默认ifchange)
        
        ifchange词向量基于全量cv工作经历，加入了领域相关实体，通过fasttext训练，没有oov问题。参考:https://fasttext.cc
        
        腾讯词向量具体信息参见：https://ai.tencent.com/ailab/nlp/embedding.html
        
        3.2.1 在线请求词向量
        
        ```python
        # type 默认'ifchange'
        nlu.w2v('深度学习')
        # 腾讯词向量
        nlu.w2v('深度学习', type='tencent')
        # 或者传入多个词
        nlu.w2v(['深度学习', '机器学习'])
        ```
        
        3.2.2 获取词向量相似的词
        
        ```python
        # 默认使用e成词向量
        nlu.sim_words('深度学习', topn=10, type="ifchange")  # 10表示最多返回10个最相似的词
        # 或者传入多个词
        nlu.sim_words(['深度学习', '机器学习'], 10, "tencent")
        ```
        
        3.2.3 获得两个词的相似度
        
        ```python
        # 使用腾讯词向量
        nlu.word_sim('深度学习', '机器学习', type='tencent')
        # 使用ifchange词向量
        nlu.word_sim('深度学习', '机器学习', type='ifchange')
        ```
        
        ## 4 句向量
        
        ### 4.1 基于词向量
        
        ```python
        nlu.s2v(['主要负责机器学习算法的研究', '训练模型、编写代码、以及其他一些工作']) # 300维
        nlu.s2v(['主要负责机器学习算法的研究', '训练模型、编写代码、以及其他一些工作'], type='tencent') # 200维
        ```
        
        返回结果：
        
        ```json
        {
            'dimention': 300,  # 维度
            'veclist': [[0.01, ...,0.56],[0.89,...,-0.08]]
        }
        ```
        
        ### 4.2 基于Bert
        
        bert向量有两个版本：
        
        1. 基于哈工大全词mask的预训练句向量表征
        
        2. 基于cv中工作经历全词mask的预训练句向量表征
        
        调用方式：
        
        ```python
        nlu.bert_vec(['主要负责机器学习算法的研究', '训练模型、编写代码、以及其他一些工作'], mode="wwm_ext")  # 哈工大版
        nlu.bert_vec(['主要负责机器学习算法的研究', '训练模型、编写代码、以及其他一些工作'], mode='cv')  # ifchange版本
        ```
        
        ### 4.3 基于SentenceBert
        
        更具语义的句子向量表征，目前的SOTA模型
        
        调用方式：
        
        ```python
        # 获取句子向量
        nlu.bert_encode("句子通用向量表征") # 向量维度512
        nlu.bert_encode(["句子通用向量表征", "自然语言处理是人工智能的明珠"])
        # 计算两个句子相似度
        nlu.bert_sim("句子通用向量表征", "自然语言处理是人工智能的明珠")
        # 计算句子集合B中与句子A最相似的句子
        nlu.bert_sim("句子通用向量表征", ["自然语言处理是人工智能的明珠", "训练模型、编写代码、以及其他一些工作"])
        # 计算两组句子间的两两相似度
        nlu.bert_sim(
            ["句子通用向量表征", "自然语言处理是人工智能的明珠"],
            ["主要负责机器学习算法的研究", "训练模型、编写代码、以及其他一些工作"])
        ```
        
        ### 5 下载预训练中文语言模型
        
        可用的模型有：
        
        * base_cn: Google官方中文Base
        * wwm: 哈工大全词MASK_v1
        * wwm_ext: 哈工大全词MASK_v2
        * ernie_cv: 使用工作经历文本重新训练的ernie模型
        
        调用方式：
        
        ```python
        # 若给定输出目录，直接进行下载
        nlu.bertmodels('wwm_ext', './bert_models')
        ```
        
        ## 6 实体
        
        实体识别(转发自图谱组)
        
        基于输入的自然文本，识别 学校(school)、职能(function)、技能(skill)、学历(degree)、专业(major)、公司(company)、证书(certificate) 七大实体
        
        ```python
        nlu.ner(["我毕业于北京大学"],'ner')
        ```
        
        返回结果:
        
        ```json
        [
            [
                {
                'type': 'school',
                'text': '北京大学',
                'boundary': [4, 8],
                'entityIdCandidates': [{'entityID': '0', 'entityName': '', 'score': 1.0}]
                }
            ]
        ]
        ```
        
        ## 7 情感分析
        
        返回句子的情感极性，持正向和负向情感
        
        参数说明：
        
        * sentences 输入的文本列表
        * prob 值为False，不返回预测句子的情感预测得分，只返回情感类别（pos或者neg）；值为True，则都返回。
        
        调用方式：
        
        ```python
        nlu.emotion(['这家公司很棒','这家公司很糟糕'], prob=False)
        ```
        
        返回结果：
        
        ```json
        {
            'text': ['这家公司很棒','这家公司很糟糕'],
            'labels': ['pos','neg']
        }
        ```
        
        ## 8 关键字提取
        
        方法：keywords(content,topk,with_weight)
        
        参数说明：
        
        * content 为输入文本.
        * topk 为最大返回关键字个数. 默认3
        * with_weight 是否返回关键字的权值. 默认False
        
        调用方式：
        
        ```python
        nlu.keywords('主要负责机器学习算法的研究以及搭建神经网络，训练模型，编写代码，以及其他的一些工作',4,True)
        ```
        
        返回结果：
        
        ```json
        {'weights': [9.64244, 9.36891, 6.2782, 5.69476], 'keywords': ['机器学习算法', '神经网络', '训练', '模型']}
        ```
        
        ## 9 句子相似度计算
        
        句子相似有2种计算方式，
        
        1. 基于ifchange词向量的句向量的cosine  
        
        2. 基于腾讯词向量的句向量的cosine
        
        方法: sent_simi(text1, text2, precision=100, type='ifchange')
        
        参数说明：
        
        * text1 为待计算句子1
        * text2 为待计算句子2
        * precision 为计算结果刻度，如1000，则返回0~1000的值
        * type : ifchange | tencent
        
        调用方式：
        
        ```python
        nlu.sent_sim('你家的地址是多少', '你住哪里', 1000, type="ifchange")
        ```
        
        返回结果:
        
        ```json
        {'result': 600}
        ```
        
        ## 10 动宾提取
        
        方法: vob(content, mode）
        
        参数说明:
        
        * content 输入文本，str
        * mode 提取模式，可选值为 fast 或accurate. 目前仅支持fast，忽略次参数
        
        调用方式：
        
        ```python
        nlu.vob('要负责机器学习算法的研究以及搭建神经网络，训练模型，编写代码，以及其他的一些工作')
        ```
        
        返回结果：
        
        ```json
        {'content': [['编写', ' 代码']]}
        ```
        
        ## 11 句子合理性判别
        
        方法： rationality(text, with_word_prob)
        
        参数说明：
        
        * text, 带判定句子,类型是list
        * with_word_prob,返回结果中是否包含每个词合理性的概率，str，取值范围为 'true' 或 'false'。 默认'false'
        
        调用方式：
        
        ```python
        nlu.rationality(['床前明月光，疑是地上霜', '床前星星光，疑是地上霜', '床前白月光，疑是地上霜'])
        ```
        
        返回结果：
        
        ```json
         {
            'ppl': [63.2965, 187.2091, 71.3999]
         }
        ```
        
        ## 12 姓名识别服务
        
        来自nb2组的姓名识别
        
        调用方式：
        
        ```python
        nlu.name_ner("刘德华的⽼老老婆叫叶丽倩")
        ```
        
        返回结果： ['刘德华', '叶丽倩']
        
        ## 13 小样本分类
        
        基于sentence-bert的小样本快速分类模型
        
        方法：infer(sent, show_dist)
        
        参数说明:
        
        * sent: 待分类的文本，字符串或列表皆可
        * show_dist: 是否展示详细分类结果, 默认True
        
        你需要准备少量的不同类别的文本作为训练语料，参考格式[example.txt](https://gitlab.ifchange.com/nlu/nlutools/blob/dev/python/test/example.txt)
        
        每一行为"文本\t标签"
        
        调用方式：
        
        ```python
        from nlutools import Classifier
        classifier = Classifier("test/example.txt")
        classifier.infer("我要上课")
        classifier.infer(["我要上课", "我要学习"])
        ```
        
Platform: UNKNOWN
Description-Content-Type: text/markdown
