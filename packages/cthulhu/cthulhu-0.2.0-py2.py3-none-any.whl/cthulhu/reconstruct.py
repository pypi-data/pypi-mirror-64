# This Source Code Form is subject to the terms of the Mozilla Public
# License, v. 2.0. If a copy of the MPL was not distributed with this
# file, You can obtain one at https://mozilla.org/MPL/2.0/.

# Python 2 and 3 compatibility
from __future__ import print_function, division

import os
import sys

import numpy as np
import scipy as sp
from scipy import stats, signal
from scipy.fftpack import fft2, fftshift
from scipy.interpolate import griddata
from scipy.optimize import curve_fit
from astropy.io import fits

from cthulhu.blacklist import get_blacklist
from pyGrad2Surf.g2s import g2s


class Obsid(object):
    """
    This class provides the means to perform ionospheric analyses.
    "data" can simply be a list or tuple of source information, specifically:
    (ra, dec, ra_shifts, dec_shifts)
    where "ra" and "dec" describe the spatial coordinates of the sources, and
    "ra_shifts" and "dec_shifts" describe the deviation from the expected
    positions of sources, presumably due to the ionosphere.

    Note that the coordinates of these positions and shifts need not be (RA, Dec);
    they may be anything, so long as they are consistent.

    Alternatively, "data" can be a dictionary generated by cthulhu, which also
    contains metadata on the observation supplied.
    """

    def __init__(self, data, obsid=None, sources=None, flux_densities=None, radius=None,
                 blacklist=True):
        # If "blacklist" is enabled, we're going to remove known extended sources.
        if blacklist:
            bad_sources = get_blacklist()
        else:
            bad_sources = []

        # Dictionaries are assumed to have at least some structure; specifically,
        # the primary key "sources" has values which are in turn more dictionaries,
        # each of which has "ra", "dec", "ra_shifts" and "dec_shifts".
        # Dictionaries can optionally have a "metadata" key at the top level, and
        # a "flux_density" key for each source.
        if isinstance(data, dict):
            try:
                if obsid is None:
                    obsid = data["metadata"]["obsid"]
            except KeyError:
                pass

            ra, dec, ra_shifts, dec_shifts = [], [], [], []
            self.flux_densities = []
            self.sources = []
            for s in data["sources"]:
                if s not in bad_sources:
                    ra.append(data["sources"][s]["ra"])
                    dec.append(data["sources"][s]["dec"])
                    ra_shifts.append(np.nanmean(data["sources"][s]["ra_shifts"]))
                    dec_shifts.append(np.nanmean(data["sources"][s]["dec_shifts"]))
                    try:
                        self.flux_densities.append(data["sources"][s]["flux_density"])
                    except KeyError:
                        pass
                    self.sources.append(s)

        # If there are four elements in the unpacked data, assume it's:
        # (ra, dec, ra_shifts, dec_shifts)
        else:
            assert len(data) == 4, "Unknown data format. Expected: (ra, dec, ra_shifts, dec_shifts)"

            self.sources = []
            self.flux_densities = []
            # If the source names have been specified, the filter them according to the blacklist.
            if sources is not None:
                ra, dec, ra_shifts, dec_shifts = [], [], [], []
                for i, s in enumerate(sources):
                    if s not in bad_sources:
                        ra.append(data[0][i])
                        dec.append(data[1][i])
                        ra_shifts.append(data[2][i])
                        dec_shifts.append(data[3][i])
                        self.sources.append(s)
                        if flux_densities is not None:
                            self.flux_densities.append(flux_densities[i])
            # No sources specified - proceed with the data as is.
            else:
                ra, dec, ra_shifts, dec_shifts = data

        # Make sure the data is in a numpy format.
        self.ra = np.array(ra)
        self.dec = np.array(dec)
        self.ra_shifts = np.array(ra_shifts)
        self.dec_shifts = np.array(dec_shifts)
        self.flux_densities = np.array(self.flux_densities)
        self.sources = np.array(self.sources)

        if obsid is not None:
            self.obsid = obsid
        else:
            self.obsid = 0

        assert len(self.ra) > 0, "No data supplied!"

        assert_error_message = "Unpacked data contains four lists, but " \
                               "they are not all the same size."
        assert len(self.ra) == len(self.dec), assert_error_message
        assert len(self.ra) == len(self.ra_shifts), assert_error_message
        assert len(self.ra) == len(self.dec_shifts), assert_error_message

        # For MWA data, many sources may lie in the sidelobes. Here, we attempt
        # to isolate sources only in the primary beam.
        # Use all the sources to find an approximate centre.
        bulk_centre_ra = np.mean(self.ra)
        bulk_centre_dec = np.mean(self.dec)

        if radius is None:
            self.determine_radius()
        else:
            self.radius = radius

        # Recalculate the centre, based on the sources within the radius,
        # and specify the sources to be used for analysis.
        filtered = np.array([[a, b, c, d] for a, b, c, d
                             in zip(self.ra, self.dec, self.ra_shifts, self.dec_shifts)
                             if abs(a-bulk_centre_ra) < self.radius
                             and abs(b-bulk_centre_dec) < self.radius])

        self.fra = filtered[:, 0]
        self.fdec = filtered[:, 1]
        self.fra_shifts = filtered[:, 2]
        self.fdec_shifts = filtered[:, 3]
        self.ra_centre = self.fra.mean()
        self.dec_centre = self.fdec.mean()

    def determine_radius(self):
        """
        Expects RA coordinates to be unwrapped! (i.e. 359 should be -1, because 359
        is *not* next to 0.)
        """
        bound = lambda x: np.max(np.abs(np.percentile(x, [2, 98]) - np.median(x)))
        ra_bounded = bound(self.ra)
        dec_bounded = bound(self.dec)
        self.radius = max(ra_bounded, dec_bounded)

    def reconstruct_tec(self, resolution=200, frequency=200, g2s_method=g2s,
                        interp_method="linear", crop=True, filtering=True):
        # If "filtering" is true, use only the sources within the radius determined early.
        # Otherwise, use all sources.
        if filtering:
            ra = self.fra
            dec = self.fdec
            ra_shifts = self.fra_shifts
            dec_shifts = self.fdec_shifts
        else:
            ra = self.ra
            dec = self.dec
            ra_shifts = self.ra_shifts
            dec_shifts = self.dec_shifts

        # Use griddata to interpolate a grid to the both the RA and Dec shifts.
        # Negatives are required for a physically realistic representation -
        # source shifts move from high surface values to low surface values.
        # Flipping left-right reflects RA increasing right-to-left.
        grid_x, grid_y = np.meshgrid(np.linspace(-self.radius, self.radius, resolution),
                                     np.linspace(-self.radius, self.radius, resolution))
        grid_x += self.ra_centre
        grid_y += self.dec_centre

        self.grid_dra = np.flipud(np.fliplr(griddata(np.vstack((ra, dec)).T, ra_shifts,
                                            (grid_x, grid_y), method=interp_method, fill_value=0)))
        self.grid_ddec = np.flipud(np.fliplr(griddata(np.vstack((ra, dec)).T, dec_shifts,
                                             (grid_x, grid_y), method=interp_method, fill_value=0)))

        # \Delta STEC [1/m^3] = \Delta\theta [rad] * \nu^2 [(MHz)^2] / k
        # k = 1/(8\pi^2) e^2/(\epsilon_0 m_e) [m^3 / s^2]
        # 1 TECU = 10^16 [1/m^2]
        constant = 40.30819  # [m^3 / s^2]
        self.grid_dra = np.deg2rad(self.grid_dra) * (frequency*1e6)**2 / constant  # [1/m^3]
        self.grid_ddec = np.deg2rad(self.grid_ddec) * (frequency*1e6)**2 / constant

        # Finally, use the specified g2s method on the interpolated grids.
        # Because the field is still specified in degrees, we are "integrating"
        # w.r.t. degrees, not metres.
        self.tec = np.flipud(g2s_method(grid_x[0, :], grid_y[:, 0], self.grid_dra, self.grid_ddec))  # [deg/m^3]
        self.tec /= 1e16  # [TECU * deg/m]

        # Remove the arbitrary "constant of integration" from the TEC.
        self.tec -= self.tec.min()

        self.tec_extent = [self.ra_centre+self.radius,
                           self.ra_centre-self.radius,
                           self.dec_centre-self.radius,
                           self.dec_centre+self.radius]
        self.tec = 400e3 * np.tan(np.deg2rad(self.tec))
        self.tec_extent_m = [grid_x.max(), grid_x.min(), grid_y.min(), grid_y.max()]

        if crop:
            self.crop_tec()


    def crop_tec(self, crop_factor=1./np.sqrt(2)):
        # Before performing any statistics, it's a good idea to mask or crop
        # the regions of the matrices that have less sources (the edges, hopefully).
        self.tec_extent = (self.ra_centre+self.radius*crop_factor,
                           self.ra_centre-self.radius*crop_factor,
                           self.dec_centre-self.radius*crop_factor,
                           self.dec_centre+self.radius*crop_factor)

        def cropper(matrix):
            length = len(matrix)
            cropped_length = int(length * crop_factor)
            i1 = int((length-cropped_length)/2)
            i2 = length-int((length-cropped_length)/2)
            return matrix[i1:i2, i1:i2]

        self.tec = cropper(self.tec)
        self.grid_dra = cropper(self.grid_dra)
        self.grid_ddec = cropper(self.grid_ddec)


    def obsid_metric(self):
        # try:
        #     self.tec
        # except AttributeError:
        #     self.reconstruct_tec()

        # try:
        #     self.non_dc_power
        # except AttributeError:
        #     self.tec_power_spectrum()

        try:
            self.pca_variance
        except AttributeError:
            self.pca()

        # self.s1 = [60*np.median(np.abs(self.fra_shifts)), "median(abs(ra_shifts)) [arcmin]"]
        # self.s2 = [60*np.median(np.abs(self.fdec_shifts)), "median(abs(dec_shifts)) [arcmin]"]
        self.s3 = [60*np.median(np.sqrt(self.fra_shifts**2+self.fdec_shifts**2)), "med(abs(offsets)) [arcmin]"]
        # self.s4 = [np.std(self.fra_shifts), "std(ra_shifts)"]
        # self.s5 = [np.std(self.fdec_shifts), "std(dec_shifts)"]
        # self.s6 = [np.std(np.sqrt(self.fra_shifts**2+self.fdec_shifts**2)), "std(RA & Dec offsets)"]
        # self.s7 = [np.std(self.tec), "std(TEC)"]
        # self.hessian = hessian(self.tec)
        # self.s8 = [np.std(np.abs(self.hessian[0, 0]) + np.abs(self.hessian[0, 1]) + np.abs(self.hessian[1, 1])), "std(hessian)"]

        # from scipy import ndimage
        # laplacian = ndimage.filters.laplace(self.tec)
        # self.s9 = [np.std(laplacian), "std($\mathcal{L}$(TEC))"]

        def pca2weight(pca):
            if pca > 0.6:
                return 64 * pca * (pca - 0.6)
            else:
                return 0
        pca_variance_weight = pca2weight(self.pca_variance[0])
        self.s10 = [self.pca_variance[0], "PCA eigenvalue"]

        # self.s11 = stats.skew(self.ra_shifts, axis=None)
        # self.s12 = stats.skew(self.dec_shifts, axis=None)
        # self.s13 = stats.kurtosis(self.ra_shifts, axis=None)
        # self.s14 = stats.kurtosis(self.dec_shifts, axis=None)
        # self.s15 = stats.skew(self.tec, axis=None)
        # self.s16 = stats.kurtosis(self.tec, axis=None)
        # self.s17 = self.non_dc_power

        self.metrics = [self.s3, self.s10]
        self.metric_weights = [25, pca_variance_weight]
        self.metric = np.sum([x[0]*y for x, y in zip(self.metrics, self.metric_weights)])


    def pca(self):
        # Principal Component analysis is heavily affected by sources with large
        # outliers in a distribution of RA/Dec ionospheric offsets. The following
        # routine should account for these.
        ra_mid = np.median(self.fra_shifts)
        dec_mid = np.median(self.fdec_shifts)
        bound = lambda x: np.mean(np.abs(np.percentile(x, [10, 90])))
        ra_bound = bound(self.fra_shifts)
        dec_bound = bound(self.fdec_shifts)
        bad_source_indices = ((self.fra_shifts < (ra_mid - 5 * ra_bound)) |
                              (self.fra_shifts > (ra_mid + 5 * ra_bound)) |
                              (self.fdec_shifts < (dec_mid - 5 * dec_bound)) |
                              (self.fdec_shifts > (dec_mid + 5 * dec_bound)))
        self.pca_rejected_indices = np.where(bad_source_indices)[0]
        good_ra_shifts = self.fra_shifts[~bad_source_indices]
        good_dec_shifts = self.fdec_shifts[~bad_source_indices]

        # RA increases to the left, make them negative.
        X = np.stack((-good_ra_shifts, good_dec_shifts), axis=1)
        # normalize the features
        X = X - np.mean(X, axis=0)
        # perform SVD
        _, S, V = sp.linalg.svd(X, full_matrices=False)

        self.pca_variance = S**2 / X.shape[0]
        self.pca_variance /= self.pca_variance.sum()

        dominant = V[0, :]
        self.pca_slope = dominant[1]/dominant[0]
        self.pca_angle = float(np.arctan2(dominant[1], dominant[0]))

        # # Code for looking at the structure of the PS along the dominant axis.
        # width = 5
        # ps = fftshift(ps)
        # freqs = fftfreq(ps.shape[0], d=((self.tec_extent[0]-self.tec_extent[1])/ps.shape[0]))
        # middle = len(ps)/2
        # i = 1
        # one_d_ps = []
        # while freqs[i] < 1:
        #     if self.pca_slope > 1:
        #         #ps[middle+i, np.rint(np.arange(-width, width)+middle+i*1/self.pca_slope).astype(int)] = 1e9
        #         one_d_ps.append(np.mean(ps[middle+i, np.rint(np.arange(-width, width)+middle+i*1/self.pca_slope).astype(int)]))
        #     else:
        #         #ps[np.rint(np.arange(-width, width)+middle+i*self.pca_slope).astype(int), middle+i] = 1e9
        #         one_d_ps.append(np.mean(ps[np.rint(np.arange(-width, width)+middle+i*self.pca_slope).astype(int), middle+i]))
        #     i += 1
        # ps = fftshift(ps)
        # self.one_d_ps = one_d_ps
        # self.freq_res = freqs[1] - freqs[0]


    def tec_power_spectrum(self):
        w = signal.blackman(self.tec.shape[0])
        w = np.outer(w, w)

        ps = np.abs(fft2(self.tec*w))**2

        def non_dc_power(ps):
            ps_shifted = fftshift(ps)
            h, w = ps_shifted.shape
            c = len(ps)/2
            x, y = np.meshgrid(np.arange(w), np.arange(h))
            d = (x - c)**2 + (y - c)**2

            r = 1

            powers = []
            while r < len(ps)/4:
                m = d < r**2
                powers.append(np.sum(ps_shifted[np.where(m)])-np.sum(powers))
                r += 1
            return np.sum(powers[3:])/np.sum(powers[0:3])

        self.ps = ps
        self.non_dc_power = non_dc_power(ps)


    def tec_residuals(self):
        matrix_length = len(self.tec)
        ra_extent = self.tec_extent[0] - self.tec_extent[1]
        dec_extent = self.tec_extent[3] - self.tec_extent[2]
        ddec, dra = np.gradient(self.tec,
                                [ra_extent/self.tec.shape[1], dec_extent/self.tec.shape[0]])

        visible = np.array([(a, b, c, d) for a,b,c,d
                            in zip(self.ra, self.dec, self.ra_shifts, self.dec_shifts)
                            if a > self.tec_extent[1] and a < self.tec_extent[0]
                            and b > self.tec_extent[2] and b < self.tec_extent[3]])
        v_ra = visible[:, 0]
        v_dec = visible[:, 1]
        v_ra_shifts = visible[:, 2]
        v_dec_shifts = visible[:, 3]

        # RA = ax + b
        # where x is the matrix coordinate
        # a = (RA extent)/(Matrix length), b = RA value @ Matrix[0]
        a = (self.tec_extent[0] - self.tec_extent[1])/matrix_length
        b = self.tec_extent[0]
        x = ((b - v_ra)/a).astype(int)
        a = (self.tec_extent[3] - self.tec_extent[2])/matrix_length
        b = self.tec_extent[2]
        y = ((v_dec - b)/a).astype(int)

        def printer(index):
            print(v_ra[index], v_dec[index],
                  x[index], y[index],
                  v_ra_shifts[index], v_dec_shifts[index],
                  dra[x[index], y[index]],
                  ddec[x[index], y[index]])
        # printer(0)
        # printer(1)
        # printer(2)
        # printer(3)

        # tec_ra_shifts = dra[y, x]
        # tec_dec_shifts = ddec[y, x]
        tec_ra_shifts = self.grid_dra[y, x]
        tec_dec_shifts = self.grid_ddec[y, x]

        self.dra = dra
        self.ddec = ddec
        self.rra = v_ra
        self.rdec = v_dec
        self.rra_shifts = v_ra_shifts + tec_ra_shifts
        self.rdec_shifts = v_dec_shifts + tec_dec_shifts


    def save_tec_fits(self, comment=None, verbosity=0, overwrite=False, filename=None, directory=None):
        if not directory:
            directory = "fits_files"
        if not filename:
            filename = directory + "/%s.fits" % self.obsid
        else:
            filename = "%s/%s" % (directory, filename)

        if not os.path.exists(directory):
            os.mkdir(directory)
        if not overwrite and os.path.exists(filename):
            if verbosity > 0:
                sys.stderr.write("Not overwriting and %s exists; no fits saved.\n" % filename)
            return
        elif overwrite and os.path.exists(filename):
            os.remove(filename)

        hdr = fits.Header()
        hdr["COMMENT"] = "Obsid: %s" % self.obsid
        if comment:
            hdr["COMMENT"] = comment
        hdr["COMMENT"] = "Generated by cthulhu"
        hdr["CTYPE1"] = "RA"
        hdr["CTYPE2"] = "DEC"
        hdr["CRVAL1"] = self.tec_extent[0]
        hdr["CRVAL2"] = self.tec_extent[2]
        hdr["CDELT1"] = (self.tec_extent[1] - self.tec_extent[0]) / self.tec.shape[1]
        hdr["CDELT2"] = (self.tec_extent[3] - self.tec_extent[2]) / self.tec.shape[0]
        hdr["BUNIT"] = "TECU"
        hdu = fits.PrimaryHDU(self.tec.astype(np.int32), header=hdr)
        hdu.writeto(filename)
        if verbosity > 0:
            print("Saved: "+filename)


def hessian(x):
    """
    Calculate the hessian matrix with finite differences
    Parameters:
        - x : ndarray
    Returns:
        an array of shape (x.dim, x.ndim) + x.shape
        where the array[i, j, ...] corresponds to the second derivative x_ij
    http://stackoverflow.com/a/31207520/6263858

    N.B. For 2D matrices (i.e. this work), hessian[0, 0] corresponds to d2/dy2, not d2/dx2.
    """
    x_grad = np.gradient(x)
    hessian = np.empty((x.ndim, x.ndim) + x.shape, dtype=x.dtype)
    for k, grad_k in enumerate(x_grad):
        # iterate over dimensions
        # apply gradient again to every component of the first derivative.
        tmp_grad = np.gradient(grad_k)
        for l, grad_kl in enumerate(tmp_grad):
            hessian[k, l, :, :] = grad_kl
    return hessian
